在2014年`Rong, X`的论文中有两点值得注意：

**第一点，“通过从输入层到隐藏层或隐藏层到输出层的权重矩阵去向量化表示词的输入” 这句说的是啥意思呢？**

当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个$x$的`one-hot encoder`: $[1,0,0,…,0]$，对应刚说的那个词语`吴彦祖`，则在输入层到隐含层的权重里，只有对应`1`这个位置的权重被激活，当前词和隐藏层的结点一一进行带权重的相乘，相乘后的结果组成一个向量$v_x$来表示$x$，而因为每个词语的`one-hot encoder`里面`1`的位置是不同的，所以，这个向量$v_x$就可以用来唯一表示$x$。
	
类似的，输出$y$也是用$V$个结点表示的，对应$V$个词语，所以其实，我们把输出结点置成$[1,0,0,…,0]$，它也能表示`吴彦祖`这个单词，但是激活的是隐藏层到输出层的权重，当前词和输出层的结点一一进行带权重的相乘，相乘后的结果组成一个向量$v_y$，跟上面提到的$v_x$维度一样，并且可以看做是词语`吴彦祖`的另一种词向量。而这两种词向量$v_x$和$v_y$，正是`Mikolov`在论文里所提到的`输入向量`和`输出向量`，一般我们用`输入向量`。

**第二点，2014年Rong, X的论文中提到，学习迭代的是两个权重矩阵(分别用$W、W'$表示)，学习$W$还好，但学习$W'$的计算量巨大，所以改用的HS或负采样。但对于喜欢刨根问底的同学来说，到底具体是怎么个`还好`，怎么个`计算量巨大`呢？**

1. 首先，整个网络过程，我们需要做的是用输入的词去预测输出的词。其中输入层的单词$w_i$使用`one-hot`来表示的, 即在上图中$x_1,x_2,x_3,\dots,x_V$只有$x_k$为1，其余为0，其中$k$可以是输入的词在词汇表中的索引下标。之后就是经过词向量矩阵$W$连接输入层和隐藏层。其中由于$X$中只有一个`1`，因此经过与$W$相乘，相当于取出$W$中的的第k行，实际也就是输入单词$w_i$的$N$维的词向量，使用$v_{wI}$表示，来作为隐藏层的值。

	注意，word2vec的隐藏层并没有激活函数：
	$h = W^{T}\cdot X = v_{wI}^{T}$
	
	然后考虑从隐层的$h$到输出层$Y$，同样$h$经过矩阵$W'$相乘，得到一个$V\times 1$的向量$u$:
	$u = W'^{T}\cdot h$

	其中$u$中的每个元素$u_j$就是$W'$的第$j$列用$v'_{wj}$表示, 与$h$做内积得到：
	$u_j = v{'}_{wO}^{T}\cdot h$
	
	含义就是词汇表中第$j$个词的分数，我们的目的就是要根据输入词$w_j$去预测输出的词，因此预测的词就取分数最高的即可。
	
	这里为了方便概率表示，使用softmax将$u$归一化到$[0,1]$之间, 从而作为输出词的概率, 其实是一个多项分布, 也就是上图中的$y$:
	$\displaystyle\boldsymbol{P}(w_j|w_I) = y_j = \frac{exp(u_j)}{\sum_{k \in V}exp(u_k)} = \frac{exp(v{'}_{wj}^T\cdot v_{wI})}{\sum_{k \in V}exp(v{'}_{wk}^T\cdot v_{wI})}$
	
	其中$v_w$与$v'_w$都称为词$w$的词向量，不过前面说过了，一般使用前者作为词向量，而非后者。

	至此前向过程完成，就是给定一个词作为输入，来预测它的上下文词，还是比较简单的，属于简化版的神经语言模型。这个过程中需要用到的参数有两个词向量矩阵$W, W'$，下面就是重点了，介绍如何根据语料库来训练模型，更新参数，得到最终的词向量。
	
2. 接着，明确训练数据的格式，对于一个训练样本$(w_i, w_o)$，输入是词$w_i$的`one-hot`编码，其维度定义为$V$的向量$x$，模型预测的输出同样也是一个维度为$V$的向量$y$，同时真实值$w_o$也是用`one-hot`表示，记为$t= [ 0,0,0 \dots 1,0,0]$，其中假设$t_{j^*}= 1$，也就是说$j^*$是真实单词在词汇表中的下标，那么根据最大似然或者上面的语言模型，目标函数可以定义如下: 
$\begin{aligned} O &= \max{\boldsymbol{P}(w_o|w_I)}\\ &= \displaystyle\max{y_{j^*}}:= \max{\log{y_{j^*}}}  \\ &=\displaystyle \max{\log{(\frac{exp(u_{j^*})}{\sum{exp(u_k)}})}} \\ &= \max{u_{j^*}} - \log{\sum_{k=1}^{V}exp(u_k)}\end{aligned}$

	一般我们习惯于最小化损失函数，因此定义损失函数:
	$\boldsymbol{E} = -u_{j^*} + \displaystyle\log{\sum_{k=1}^{V}exp(u_k)}$
	
	然后结合反向传播一层层求梯度，使用梯度下降来更新参数。先求隐藏层到输出层的向量矩阵$W'$的梯度：
